
workflow
    part 1/2
        long term net recognizes weights for different activities
            generates weights for activities?
        short term net helps to explore
    parrt ?
        adversarial learning?
            make it a game for himself
        learn only from a small couple of games
            overfit hard to the first
                do not repeat moves
            more general from next games
                assume repeated action sequences have same reward
                    do not explore them
                similar action sequences different though


method
    play 10 episodes
        high exploration
        overfit to these experiences
            store whole game until good reward or done
            adjust rewards, values etc
            repeated random sample
    start training
        high exploration until find good reward
        duplicate paths with high exploraton at end?

    reward function = fn(reward) + step
        encourage longer games
            once longest game reached then it will learn optimal policy

    learns using a sequence of actions
        dynamic lstm??
        something better?

    limit to exploration
        if no reward from exploration path stop going down that path

    add "boredom"
        agent doent want to do something with no result/repeat for too long

    generalized overfitting
        see one picture of cat from front
        overfit and generalize by creating more input from the one sample
    
    at start intermediate net pulls weights from long net 
        (long net predicts weights?)
        (long just stores a state, reward, weights)
        (predicts which one based on state similarities)

